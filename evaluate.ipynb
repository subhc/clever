{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a152872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def eval_fgsm(root = '.', dataset = 'CUB', corpus_name = 'aab'):\n",
    "\n",
    "    class SiameseFCN(nn.Module):\n",
    "        def __init__(self, shared_in_features=1024, shared_out_feature=64, num_labels=2):\n",
    "            super(SiameseFCN, self).__init__()\n",
    "            self.shared_fc = nn.Sequential(\n",
    "                nn.Linear(shared_in_features, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, shared_out_feature),\n",
    "                nn.ReLU())\n",
    "            self.classifier_in_features = 3 * shared_out_feature\n",
    "            self.classifier = nn.Linear(self.classifier_in_features, num_labels)\n",
    "\n",
    "        def forward(self, rep_a, rep_b):\n",
    "            rep_a = self.shared_encode(rep_a)\n",
    "            rep_b = self.shared_encode(rep_b)\n",
    "\n",
    "            return self.get_classifier_scores(rep_a, rep_b)\n",
    "\n",
    "        def shared_encode(self, x):\n",
    "            return self.shared_fc(x)\n",
    "\n",
    "        def get_classifier_scores(self, rep_a, rep_b):\n",
    "            return self.classifier(torch.cat((rep_a, rep_b, torch.abs(rep_a - rep_b)), 1))\n",
    "\n",
    "    device = 'cuda'\n",
    "    chk = f'{root}/checkpoints/stage_two/{dataset.lower()}.pth'\n",
    "\n",
    "    model = SiameseFCN(shared_in_features=1024, shared_out_feature=32, num_labels=3)\n",
    "    model.load_state_dict(torch.load(chk)[\"state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval();\n",
    "\n",
    "    def accuracy(output, target, present_classes, topk=(1, 5)):\n",
    "        \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "        with torch.no_grad():\n",
    "            maxk = max(topk)\n",
    "\n",
    "            _, pred = output.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "            res = []\n",
    "            for k in topk:\n",
    "                correct_k = correct[:k].float().sum(0)\n",
    "                res.append(compute_per_class_metric(correct_k, target, present_classes))\n",
    "            return [r.item() * 100. for r in res]\n",
    "\n",
    "    def rank(output, target, present_classes):\n",
    "        \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "        with torch.no_grad():\n",
    "            pred = output.argsort(dim=1, descending=True)\n",
    "\n",
    "            rank = pred.eq(target.view(-1, 1).expand_as(pred)).nonzero(as_tuple=False)[:, 1].float()\n",
    "            return compute_per_class_metric(rank + 1, target, present_classes).item()\n",
    "\n",
    "    def compute_per_class_metric(metric, target, present_classes):\n",
    "        acc_per_class = 0.\n",
    "        for i in present_classes:\n",
    "            idx = (target == i)\n",
    "            e = torch.true_divide(torch.sum(metric[idx]), torch.sum(idx))\n",
    "            acc_per_class += e\n",
    "        return acc_per_class / len(present_classes)\n",
    "\n",
    "    with open(f'{root}/datasets/{dataset}/FGSM/captions_gt_features.pickle', 'rb') as handle:\n",
    "        gt_features = pickle.load(handle)\n",
    "    with open(f'{root}/datasets/{dataset}/FGSM/captions_prediction_trainval_sup_sat_features.pickle', 'rb') as handle:\n",
    "        sat_features = pickle.load(handle)\n",
    "    with open(f'{root}/datasets/{dataset}/FGSM/captions_prediction_trainval_sup_aoanet_features.pickle', 'rb') as handle:\n",
    "        aoanet_features = pickle.load(handle)\n",
    "        \n",
    "    \n",
    "    with open(f'{root}/datasets/{dataset}/FGSM/corpus_{corpus_name}_cleaned_features.pickle', 'rb') as handle:\n",
    "        corpus_features = pickle.load(handle)\n",
    "\n",
    "    data = json.load(open(f'{root}/datasets/{dataset}/image_data.json', 'r'))\n",
    "    test_images = [data['images'][i] for i in data['supervised_test_loc']]\n",
    "    test_classes = sorted(corpus_features.keys())\n",
    "    test_corpus_features = [corpus_features[c].to(device) for c in test_classes]\n",
    "\n",
    "    def cartesian_classifier_on_embeddings(model, feat_a, feat_b):\n",
    "        output = model.get_classifier_scores(torch.repeat_interleave(feat_a, repeats=feat_b.size(0), dim=0), feat_b.repeat(feat_a.size(0), 1))\n",
    "        return output.view(len(feat_a), len(feat_b), -1)\n",
    "\n",
    "    document_list = test_corpus_features\n",
    "    document_lengths = [len(docs) for docs in document_list]\n",
    "    document_lengths = [0] + np.cumsum(document_lengths, 0).tolist()\n",
    "    document_spans = [(document_lengths[i], document_lengths[i + 1]) for i in range(len(document_lengths) - 1)]\n",
    "    assert all(docs.size(0) == end - start for docs, (start, end) in zip(document_list, document_spans))\n",
    "    document_list = torch.cat(document_list, dim=0)\n",
    "\n",
    "    def reduce_docs(doc_scores):  # [ batch x num_caps x num_sents ] x docs\n",
    "        x = [doc.mean(dim=2, keepdim=True) for doc in doc_scores]  # [ batch x num_caps x 1 ] x docs\n",
    "        return x\n",
    "\n",
    "    gt = []\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, image_detail in enumerate(test_images):\n",
    "            caption_feat_input = torch.cat([sat_features[image_detail['id']],\n",
    "                                   aoanet_features[image_detail['id']]\n",
    "                                   ], dim=0).to(device)\n",
    "            sentence_embs = model.shared_fc(caption_feat_input).unsqueeze(0)\n",
    "\n",
    "            batch, num_captions, _ = sentence_embs.size()\n",
    "            sentence_embs = sentence_embs.view(-1, sentence_embs.size(2))\n",
    "            document_embeddings = model.shared_fc(document_list)  # num_docs*num_sents x siamese_dim\n",
    "            doc_scores = cartesian_classifier_on_embeddings(model, sentence_embs, document_embeddings)  # batch*num_captions x num_docs*num_sents\n",
    "            doc_scores = [doc_scores[:, start:end].view(batch, num_captions, end - start, doc_scores.size(2)) for start, end in document_spans]  # [ batch x num_caps x num_sents ] x num_docs\n",
    "            doc_scores = [doc.softmax(3) for doc in doc_scores]  # [ batch x num_caps x num_sents ] x num_docs\n",
    "            doc_scores = [doc[:, :, :, 1] - doc[:, :, :, 0] for doc in doc_scores]  # [ batch x num_caps x num_sents ] x num_docs\n",
    "            doc_scores = reduce_docs(doc_scores)  # [ batch x num_caps ] x num_docs\n",
    "            doc_scores = [doc.mean(dim=1) for doc in doc_scores]  # [ batch x num_caps x num_sents ] x num_docs\n",
    "            doc_scores = torch.cat(doc_scores, 1)  # batch x num_docs\n",
    "            doc_scores = doc_scores.softmax(1)\n",
    "\n",
    "            gt.append(test_classes.index(image_detail['class_name']))\n",
    "            pred.append(doc_scores)\n",
    "            \n",
    "    gt = np.array(gt)\n",
    "    pred = torch.cat(pred, 0).cpu()\n",
    "    acc = accuracy(output=pred, target=torch.from_numpy(gt), present_classes=range(len(test_classes)), topk=(1, 5))\n",
    "    mr = rank(output=pred, target=torch.from_numpy(gt), present_classes=range(len(test_classes)))\n",
    "\n",
    "    print(f'[{dataset}] [{corpus_name}] top-1: {acc[0]:.2f} top-5: {acc[1]:.2f} mean_rank: {mr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9d221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUB] [aab] top-1: 7.87 top-5: 28.57 mean_rank: 31.92\n",
      "[FLO] [wiki] top-1: 6.24 top-5: 14.19 mean_rank: 39.70\n"
     ]
    }
   ],
   "source": [
    "eval_fgsm(dataset = 'CUB', corpus_name = 'aab')\n",
    "eval_fgsm(dataset = 'FLO', corpus_name = 'wiki')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
